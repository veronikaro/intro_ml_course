{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "import random\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk import word_tokenize\n",
    "from torch import optim\n",
    "\n",
    "random_state = random.getstate()\n",
    "batch_size = 64\n",
    "\n",
    "text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize)\n",
    "qid = torchtext.data.Field()\n",
    "target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "train = torchtext.data.TabularDataset(path='../input/train.csv', format='csv',\n",
    "                                      fields={'question_text': ('text',text),\n",
    "                                              'target': ('target',target)})\n",
    "\n",
    "\n",
    "text.build_vocab(train, min_freq=1)\n",
    "text.vocab.load_vectors(torchtext.vocab.Vectors('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'))\n",
    "print(text.vocab.vectors.shape)\n",
    "\n",
    "\n",
    "train, val = train.split(split_ratio=0.8, random_state=random_state)\n",
    "\n",
    "\n",
    "train_iter = torchtext.data.BucketIterator(dataset=train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sort_key=lambda x: x.text.__len__(),\n",
    "                                           shuffle=True,\n",
    "                                           sort=False)\n",
    "\n",
    "val_iter = torchtext.data.BucketIterator(dataset=val,\n",
    "                                         batch_size=batch_size,\n",
    "                                         sort_key=lambda x: x.text.__len__(),\n",
    "                                         train=False,\n",
    "                                         sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, loss_func, optimizer, train_iter, val_iter):\n",
    "    step = 0\n",
    "    train_record = []\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        train_iter.init_epoch()\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            model.train()\n",
    "            x = train_batch.text.cuda()\n",
    "            y = train_batch.target.type(torch.Tensor).cuda()\n",
    "            model.zero_grad()\n",
    "            pred = model.forward(x).view(-1)\n",
    "            loss = loss_function(pred, y)\n",
    "            loss_data = loss.cpu().data.numpy()\n",
    "            train_record.append(loss_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Step: {:06}, loss {:.4f}\".format(step, loss_data))\n",
    "            if step % 10000 == 0:\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                val_loss = []\n",
    "                for val_batch in iter(val_iter):\n",
    "                    val_x = val_batch.text.cuda()\n",
    "                    val_y = val_batch.target.type(torch.Tensor).cuda()\n",
    "                    val_pred = model.forward(val_x).view(-1)\n",
    "                    val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())\n",
    "                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n",
    "                print('Epoch {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n",
    "                            e, step, np.mean(train_record), val_record[-1]['loss']))\n",
    "                train_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx, hidden_dim, static=True):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        # TODO\n",
    "    \n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        # TODO \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(text.vocab.vectors,\n",
    "                    padding_idx=text.vocab.stoi[text.pad_token],\n",
    "                    hidden_dim=128).cuda()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=1e-3)\n",
    "\n",
    "training(model=model,\n",
    "         epoch=20,\n",
    "         loss_func=loss_function,\n",
    "         optimizer=optimizer,\n",
    "         train_iter=train_iter,\n",
    "         val_iter=val_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
